---
title: "TimeToEventData"
output: html_document
date: "2024-05-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

In the manuscript, "Repeatability and intra-class correlations from time-to-event data: towards a standardized approach" we describe a method to quantify variances and intra-class correlations (repeatability) from time-to-event data. In this supplementary materials page we demonstrate the use of this method with real data in 3 worked examples. We additionally provide the data and code used in the case study section of the manuscript. Lastly, we present the methods and results for a simulation study to illustrate the relationship among model estimates (beta, random effect variance and intra-class correlation values) derived from 4 different models and implementations, run on data simulated with varying random effect variance, time intervals, and amount of censored data.

## Worked Examples

Time-to-event data range widely in the number of individuals (or other cluster variable) as well as the number of repeated measures on each individual. Furthermore, the presence and amount of censored data can vary. We use openly available data to present 3 worked examples that span a range of sample sizes and amount of censoring.

### Example 1: Frog aggression towards a simulated intruder

Our first example data set is from Peignier et al. 2022. They investigated whether *A. femoralis* frogs show repeatable aggressive behavior by measuring the latency of territorial males to approach a simulated intruder. Their sample size included 51 males with 3.2 $\pm$ 1.31 trials each. Censored data are present (6% of trials) because some males did not approach the simulated intruder. These individuals were given a ceiling value of 300 seconds (the longest trial time). To evaluate repeatability, the authors log-transformed this variable and used the Gaussian family in the rpt function (Stoffel et al. 2017).

In this first worked example, we give a high level of detail on the methods and model types described in the paper. In the subsequent examples, we present the data and run the analysis while elaborating less on the intermediary or alternative steps.

```{r E1 - load the data, echo=FALSE}

# Data from Peignier et al. 2022
fdata = read.csv("https://raw.githubusercontent.com/kelseybmccune/Time-to-Event_Repeatability/main/data/Frog_personality.csv")
fdata = fdata[which(fdata$sex == "m"),c(1,2,9)] # Only males participated in the aggression test, we only need columns for ID, repetition and latency to approach the simulated intruder. Authors did not include fixed effects, so we won't either.
fdata = fdata[!is.na(fdata$jump_s),] # Unclear what the NAs represent, but the authors remove these data, so we will too.
fdata$event = ifelse(fdata$jump_s==300, 0,1) # Create event column that indicates whether the event occurred or data were censored for that trial.

head(fdata,20)
```

This is a classic (un-exploded) format for animal personality data. 

#### ICC from the coxme function with the classic data set
We will first conduct a Cox proportional hazards model using the `coxme` package in R (Therneau, 2022). The `coxme` function is able to estimate multiple random effects, as well as random slopes. We included a random effect for individual identity as in Equation 3 of the main text, except that there are no fixed effects in this frog model. In this model specification, the response variable is a "survival" object that combines latency to jump ("jump_s") and the event indicator.

```{r E1 - coxme continuous model, echo = F}
library(coxme)
library(survival)
c1 = coxme(Surv(jump_s,event)~1+(1|ID), data = fdata)

var <- VarCorr(c1)$ID[[1]] # Random effect variance is 0.40
```

We can now estimate the ICC using Equation 7 from the main text. Because of the equivalence of a mixed-effects Cox proportional hazards model with the binomial generalized linear mixed-effects model with the clog-log link, Equation 7 uses the random effect variance from the Cox model and the binomial GLMM distribution-specific residual variance estimator (Nakagawa et al. 2017).

```{r E1 - Parametric ICC calculation, echo = F}
var/(var + pi^2/6)
# 0.19
# Original paper reports a repeatabiltiy of 0.24 (CI: 0.07-0.40)

```

This yields an ICC value of 0.19.

We created two R functions to estimate the p-value and a confidence interval for time-to-event ICCs calculated using `coxme`.
```{r E1 - p-value and ci functions}
# Define the function
# missing values ignored

#' @title comxe_pval
#' @description This function calculates the p-value of the effect of the random effect in a coxme model. It also provides the p-value of the effect of the random effect using a bootstrapped method.
#' @param model A coxme model object
#' @param data The original data used to fit the model
#' @param boot Number of simulations to run to produce 95 percent confidence intervals for I2. Default is \code{NULL}, where only the point estimate is provided.
#' @return A vector of p-values
#' @author Shinichi Nakagawa - s.nakagawa@unsw.edu.au
#' @author etc

coxme_pval <- function(model, data, boot = NULL) {
  # Get the original data
  
  if(all(class(model) %in% c("coxme")) == FALSE) {stop("Sorry, you need to fit a coxme model of class coxme")}
  
  # I think we need to use get the dimension of the data

  response <- as.data.frame(model$y[,1:2])
  
  fixed_formula <- as.formula(model$formulaList$fixed)
  
  # fit the model without any random effects
  fit <- survival::coxph(as.formula(fixed_formula), data = data)
  # loglikelihood ratio test
  # this is p value of effect of taking all random effects
  pval<- anova(fit, model)$P[-1]
  names(pval) <- "liklihood_ratio_test"
  
  if(!is.null(boot)){
  
  # we need to use replicate to create many vectors of these - randomize the data
  orders <- replicate(boot, sample(1:nrow(response)))  
  
  fixed_formula <- as.character(fixed_formula)  
  random_formula <-  as.vector(as.character(model$formulaList$random))        
  formula <- as.formula(paste("Surv(new_time, new_status)", 
                           "~", 
                           fixed_formula[3], 
                           "+",
                           paste(random_formula, collapse = "+")))
  data2 <- data

  # randomizaton/permutation tests
  pb <- progress::progress_bar$new(total = boot,
                                 format = "Bootstrapping [:bar] :percent ETA: :eta",
                                 show_after = 0)

  # loop
  num <- length(summary(model)$random$variance)

  store <- matrix(NA, nrow = num, ncol = boot)

  # Loop over the number of bootstraps
  for (i in 1:boot) {
  # Permute the data

  data2$new_time <- response$time[orders[ ,i]]
  data2$new_status <- response$status[orders[ ,i]]

  # Fit the original coxme model
  temp  <- tryCatch(coxme(formula, data = data2))

  # get variance component
   store[ ,i] <- summary(temp)$random$variance

   pb$tick()
   Sys.sleep(1 / boot)

    }
   
  # getting the p value
   pval2 <- sapply(1:num, function(x) {
     sum(store[x,] > summary(model)$random$variance[x])/boot}
     )
   
   names(pval2) <- paste(rep("bootstrapped_pval", num), 1:num, sep = "_")
  }

  if(exists("pval2")) {
    
  res <- c(pval, pval2)
  return(res)
  
  } else {
  res <- pval
  return(res)
  }

}


#' @title coxme_icc_ci
#' @description This function calculates the 95 percent confidence interval for the intraclass correlation from the `coxme` objects.
#' @param model A coxme model object
#' @param upper.multiplier The multiplier for the upper bound of the confidence interval. Default is 10 (adjust to a higer value if the upper bound is not reached).
#' @return A vector of the lower, point estimate, and upper bounds of the 95 percent confidence interval for the intraclass correlation
#' @author Shinichi Nakagawa - s.nakagawa@unsw.edu.au
#' @author etc

coxme_icc_ci <- function(model, upper.multiplier = 10) {
  if(all(class(model) %in% c("coxme")) == FALSE)
    {stop("Sorry, you need to fit a coxme model of class coxme")} 
  if(any(length(summary(model)$random$variance) > 1)) {stop("Sorry. At the moment, we can only have a model with one random effect.")}
  
  # Define a sequence of variance values``
  # the length of the response
  n <- nrow(model$y)
  cut = 100
  
  var_point <- summary(model)$random$variance
  
  # based on this pdf: https://cran.r-project.org/web/packages/coxme/vignettes/coxme.pdf
  # upper CI is limited to var_point*(10*log(n)) - so this could fail
  estvar1 <- seq(0.00000000000001, var_point, length = cut)
  estvar2 <- seq(var_point, var_point*upper.multiplier, length = cut+1)[-1]
  estvar <- c(estvar1, estvar2)
  
  # Initialize a vector to store the log-likelihood values
  loglik <- double(cut)
  
  # Loop over the variance values
  for (i in seq_len(cut*2)) {
    # Fit a coxme model with fixed variance
    tfit <- update(model, vfixed = estvar[i])
    
    # Compute the log-likelihood
    loglik[i] <- 2 * diff(tfit$loglik)[1]
  }
  
  # Compute the threshold for the 95% confidence interval
  temp <-  as.numeric(2 * diff(model$loglik)[1]) - loglik
  
  # Find the variance values that correspond to the threshold
  # getting lower and upper CI using profile likelihood
  lower <- approx(temp[1:(cut)], sqrt(estvar[1:(cut)]), qchisq(.95, 1))$y
  upper <- approx(temp[(cut + 1):(2*cut)], sqrt(estvar[(cut + 1):(2*cut)]), qchisq(.95, 1))$y
  
  # Return the 95% confidence interval
  ICC_lower <- lower^2 / (lower^2 + pi^2 / 6)
  if (is.na(ICC_lower)) {
    ICC_lower <- 0
  }
  ICC_point <- var_point / (var_point + pi^2 / 6)
  ICC_upper <- upper^2 / (upper^2 + pi^2 / 6)
  names(ICC_lower) <- "lower"
  names(ICC_point) <- "ICC"
  names(ICC_upper) <- "upper"
  
  return(c(ICC_lower, ICC_point, ICC_upper))
}


```


```{r E1 - Frog p-value and ci}
coxme_pval(c1,fdata,boot=100) # as there were no fixed effects in our model, the likelihood ratio test returns an NA value. Bootstrapping indicates a p-value of 0.01.
coxme_icc_ci(c1, upper.multiplier = 10) # confidence interval = 0.06 - 0.37

```


#### ICC from coxme with an exploded data set
To check the equivalence in time-to-event ICC values between Cox proportional hazards models and binomial GLMM, we need a data set that includes discrete time intervals. This is referred to as an "exploded" data set (see Fig. 1 in the main text) and we can use the function survSplit in the survival package (Therneau, 2024) to easily transform the data. In a simulation study (see below), we found no difference in model estimates or ICC values when we used 2 or 4 time intervals for the exploded data sets.

```{r E1 - explode the data, echo=F}
# decide on the intervals
psych::describe(fdata$jump_s)

fdata_int = survSplit(Surv(jump_s,event) ~ repetition+ID, data = fdata, 
                      cut = 150, start = "tstart",end = "tstop") # two time intervals

head(fdata_int,20)
```
In the exploded data set with two time intervals, for each individual and trial we have a time interval that ranges from 0 to 150 and 150 to 300 (the ceiling latency value for this data set). If the event occurs, the tstop value represents the time at which the frog approaches the speaker.


Now we can compare the Cox model output between the classic and the exploded data set. The response variable is again a "survival" object, but now combines the start and stop time of the interval and the event indicator.
```{r E1 - coxme interval model, echo = F}
c1_int = coxme(Surv(tstart,tstop,event)~1+(1|ID), data = fdata_int)

var <- VarCorr(c1_int)$ID[[1]] # Random effect variance is still 0.40
```

You can see that the variance estimate for the random effect is the same in both Cox models.


In Equation 5 of the main text we show the calculation for the non-parametric version of the ICC, Kendall's tau, which assumes a Gamma distribution of the random effect. In addition to being non-parametric, another limitation is that it is not feasible to estimate the non-parametric ICC when there are more than one random effects. We wrote a function to obtain the non-parametric ICC numerically:

```{r E1 - Non-parametric ICC function, echo = F}
# function obtained from
# https://github.com/cran/parfm/blob/master/R/frailtydistros.R

g <- function(w, k, s, sigma2) {
  -k * w + exp(w) * s + w ^ 2 /  (2 * sigma2)
}

g1 <- function(w, k, s, sigma2) {
  -k + exp(w) * s + w / sigma2
}

g2 <- function(w, k, s, sigma2) {
  exp(w) * s + 1 / sigma2    
} 

Lapl <- Vectorize(function(s, k, sigma2) {
  # Find wTilde = max(g(w)) so that g'(wTilde; k, s, theta) = 0
  WARN <- getOption("warn")
  options(warn = -1)
  wTilde <- optimize(f = g, c(-1e10, 1e10), maximum = FALSE,
                     k = k, s = s, sigma2 = sigma2)$minimum
  options(warn = WARN)
  
  # Approximate the integral via Laplacian method
  res <- (-1) ^ k * 
    exp(-g(w = wTilde, k = k, s = s, sigma2 = sigma2)) /
    sqrt(sigma2 * g2(w = wTilde, k = k, s = s, sigma2 = sigma2))
  return(res)
}, 's')

intTau <- Vectorize(function(x, intTau.sigma2=sigma2) {
  res <- x * 
    Lapl(s = x, k = 0, sigma2 = intTau.sigma2) *
    Lapl(s = x, k = 2, sigma2 = intTau.sigma2)
  return(res)
}, "x")

fr.lognormal <- function(k,
                         s,
                         sigma2,
                         what = "logLT") {
  # if (!(is.numeric(sigma2) && (sigma2 > 0)))
  # stop("The parameter sigma2 is not a positive value.")
  
  if (what == "logLT") {
    # if (!(is.numeric(s) && (s > 0)))
    #     stop("The parameter s is not positive.")
    # Find wTilde = max(g(w)) so that g'(wTilde; k, s, theta) = 0
    WARN <- getOption("warn")
    options(warn = -1)
    wTilde <- nlm(f = g, p = 0, k = k, s = s, sigma2 = sigma2)$estimate
    options(warn = WARN)
    
    # Approximate the integral via Laplacian method
    res <- -g(w = wTilde, k = k, s = s, sigma2 = sigma2) -
      log(sigma2 * g2(w = wTilde, k = k, s = s, sigma2 = sigma2)
      ) / 2
    return(res)
  }
  else if (what == "tau") {
    intTau <- Vectorize(function(x, intTau.sigma2=sigma2) {
      res <- x * 
        Lapl(s = x, k = 0, sigma2 = intTau.sigma2) *
        Lapl(s = x, k = 2, sigma2 = intTau.sigma2)
      return(res)
    }, "x")
    
    tauRes <- 4 * integrate(
      f = intTau, lower = 0, upper = Inf, 
      intTau.sigma2 = sigma2)$value - 1
    return(tauRes)
  }
}

```

```{r E1 - Non-parametric ICC estimate, echo=, warning=FALSE}
c1_ph = coxph(Surv(jump_s, event)~1 + frailty(ID, distribution="gamma"), data=fdata)
var <- c1_ph$history$`frailty(ID, distribution = "gamma")`[1] # Random effect variance is 0.35, close to that from coxme

fr.lognormal(k,s,var,what = "tau") # $ICC_{np}$ is 0.11
```
The non-parametric ICC ($ICC_{np}$) estimate for these frog data is 0.11.


#### ICC from binomial GLMM with the exploded data set
We now demonstrate the use of the binomial GLMM to analyze the exploded survival data. However, we also need to include a column as an identifier of the time interval to include as a fixed effect, which takes the place of the start and stop time from the Cox models.

```{r E1 - binomial GLMM}
# we previously told the survSplit function to create 2 intervals by splitting the latency values at 150.
# create column to identify intervals
fdata_int$interval = NA
fdata_int$interval =  ifelse(fdata_int$tstart == 0,1,2)

# Run the binomial glmm model
library(lme4)
b1 <- glmer(event ~ as.factor(interval) + (1|ID), data=fdata_int,
                 family = binomial(link="cloglog"), nAGQ=7)
var <- b1@theta^2 # extract the random effect variance from the model output object
var/(var + pi^2/6)

fr.lognormal(k,s,var,what = "tau")
```

The random effect variance from the binomial GLMM is 0.24, and the ICC value using Equation 7 is 0.13.




### Example 2: Christmas tree worm re-emergence after a simulated predator attack

These data come from Pezner et al. 2017 <https://academic.oup.com/beheco/article/28/1/154/2453511>. They tested the consistency of hiding time (latency to re-emerge from the hole) within and across days, and the impact of social environment. They tested 30 worms, each received 4 trials within a day and 4 days of sampling for a total of 16 trials per worm. Censored data are not explicitly stated, but there are 5 NA values from 2 individuals that we here assume are censored data (\~1% of data). In the original paper, the authors log-transformed the hiding time variable, used a linear mixed model to get variance estimates, then hand-calculated repeatability.

```{r E2, echo=F}
## Load the data
ctw = read.csv("CTWemergence.csv")
# "HT" variable indicates hiding time, or the latency to emerge; "Whorls" is a visual indicator of age

# 2 individuals have NA values, but it is not explained why. 
# I'll assume these are censored (the worm didn't emerge in the trial time) and give ceiling value of one unit after other highest value
ctw$event = ifelse(is.na(ctw$HT),0,1)
ctw$HT[which(is.na(ctw$HT))]<- 375

## Explode the data
# decide on the intervals
psych::describe(ctw$HT)
ctw_int = survSplit(Surv(HT,event) ~ Whorls + Trial_Total + Worm_ID, data = ctw, 
                    cut = c(50,100,150,200,250,300,350,400), start = "tstart",end = "tstop")

## Cox model and ICC calculation
ctw.cox.int = coxme(Surv(tstart,tstop, event)~Whorls + (1|Worm_ID), data=ctw_int)

var <- VarCorr(ctw.cox.int)$Worm_ID[[1]]
var/(var + pi^2/6) #0.33
# Original paper finds across day hiding time repeatability is 0.42
```

### Example 3: Repeatability of distance to cache a seed in smalll mammals

Brehm et al. 2019 (<https://onlinelibrary.wiley.com/doi/full/10.1111/ele.13324>) evaluated seed caching behavior in relation to multiple personality traits and habitat characteristics. They offered artificial seeds for small mammals to cache that had reflective flags attached so that the cached seeds could be relocated and the distance from the food platform could be measured. The data include 31 unique deer mice that cached between 1-12 seeds (mean = 3.31 ± 0.5). Understandably, it can be hard to relocate the cached seeds and so 35% of the caching distances were censored (seeds lost to follow up).

```{r E3, echo = F}
## Load the data
setwd("/Users/kelseymccune/Documents/GitHub/Time-to-Event_Repeatability/data")
data<-read.csv("data_seed_pers.csv")
pm<-subset(data, SPP=="PM") # only one species

pmdistmov<-subset(pm,REMOVE==1) # distance the seed is dispersed, only looking at seeds that were removed from the feeding platform
pmdistmov<-subset(pmdistmov, CONS!=1) # remove rows where the seed was consumed close by the feeding platform
pmdistmov$RECOVERED..Y.N.[which(pmdistmov$DIST..MOVED==15)]<-"Y" # one row seems to indicate the cached seed was found 15m from the feeding platform, but the categorical variable for whether it was removed is an NA 
table(pmdistmov$RECOVERED..Y.N.) # how many cached seeds did they recover (i.e., how much censored data is there?)
# 35% of data are censored
dist = pmdistmov[-which(pmdistmov$ID == "UNK"),c(1,5,6,14,17)] #simplify data frame
# the Recovered column is analogous to the event variable in survival analysis. Modify it to be an integer
dist$event = ifelse(dist$RECOVERED..Y.N.== "Y",1,0)
dist$event = ifelse(is.na(dist$event),0,dist$event)
dist$DIST..MOVED[which(is.na(dist$DIST..MOVED))]<-1038 # give ceiling value to NAs
dist.cox = coxme(Surv(DIST..MOVED, event) ~ TRT + (1|ID), data=dist)
summary(dist.cox)
coxme_pval(dist.cox,dist,boot=100)
coxme_icc_ci(dist.cox) # lower bound of CI is NA because ICC is small (0.05)?


## Explode the data
# Create intervals based on the number of events in each interval. Here, 25% of the data are in each of 4 intervals.
quantile(dist$DIST..MOVED, probs = seq(0,1,0.25),na.rm=T)

psych::describe(dist$DIST..MOVED)

dist_int = survSplit(Surv(DIST..MOVED, event) ~ ID, data = dist, 
                     cut = c(50,146,331.5), start = "tstart",end = "tstop")

## Cox model and ICC calculation
dist.cox.int = coxme(Surv(tstart,tstop, event) ~ 1 + (1|ID), data=dist_int)
var <- VarCorr(dist.cox.int)$ID[[1]] 
var/(var + pi^2/6) # 0.05
# Original paper did not run this particular analysis.
```

### Data and code for Case Study in manuscript

```{r Case Study, echo = F}

#Latency to solve a door on a puzzle box
jsolv = read.csv("jaySolveData.csv")
jsolv = jsolv[,c(2:6,9)]
colnames(jsolv)[6] = "Time"
jsolv$olre = factor(1:68)

solv.su = coxme(Surv(Time, Solve)~Treatment + (1|ID), data=jsolv)
summary(solv.su) 
coxme_pval(solv.su,jsolv,boot = 100)
coxme_icc_ci(solv.su) # adjusted ICC

solv.su2 = coxme(Surv(Time,Solve) ~ 1 + (1|ID), data = jsolv)
summary(solv.su2) 
coxme_pval(solv.su2,jsolv,boot = 100)
coxme_icc_ci(solv.su2) # unadjusted ICC

```

## Simulations (draft)

We conducted a small simulation study to examine whether model estimates are consistent across four different models and implementations: (1) a Cox proportional hazards model with a normal distribution of the random effect using the `coxme` function from the `coxme` package, (2) a Cox proportional hazards frailty model with a normal distribution of the random effect using `coxph` from the `survival` package, (3) a Cox proportional hazards frailty model with a gamma distribution of the random effect using `coxph` from the `survival` package, and (4) a binomial generalized linear mixed-effects model with a clog-log link using the `glmer` function from the `lme4` package. The models (1), (2) and (3) are described in Equation 3 and model (4) is described in Equation 7 in the main text.

Time-to-event data were simulated with a single predictor variable (sex) and a single random effect (i.e. cluster or individual) $\alpha$, as described in the main text. We considered $n=1000$ observations and $n_{cluster}=100$ clusters (i.e. individuals). To evaluate performance of the above models, we simulated different data sets where we varied the magnitude of the random effect variance ($\sigma^2_\alpha$) from 1 to 3, the percent of censored data was either 0 or 0.15, and the number of time intervals was either 2 or 4. We ran each model on each data set 1000 times. We excluded results if at least one of the models did not converge or had a convergence warning message.

We present illustrative plots comparing model performance when the random effect variance is set to 1 or 2, censoring = 0.15 and time intervals = 2 or 4. These simulations had the least convergence problems or warnings out of the 1000 iterations. 

Examining the model performance within each plot, we found that the beta estimates of the predictor variable ($\beta_{\text{est}}$) are identical and relatively unbiased across all models. The variance estimates of the random effect ($\sigma^2_{f_{\text{est}}}$) are very similar for the `coxme`, `coxph`, and `glmm` models, while the `coxph_gamma` model consistently underestimates this variance. Additionally, the intra-class correlation coefficient (ICC) is consistently higher in the `glmm` model compared to the `coxme` and `coxph` models. This aligns with Figure 2 in the main text, which shows that as the variance of the random effect increases, the $ICC$ value from the GLMM model becomes proportionally larger than the non-parametric value ($ICC_{np}$) from the Cox models. Furthermore, comparing performance across plots demonstrates that the number of time intervals in the data (2 or 4) has no effect on the model estimates. 

![](images/plot_scenario_4.png)
![](images/plot_scenario_10.png)

![](images/plot_scenario_5.png)
![](images/plot_scenario_11.png)

The simulation study was conducted with R (R Core Team, 2024) and using the computational cluster Katana at UNSW Sydney (UNSW, 2024).

## References

Bates, D., Maechler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. *Journal of Statistical Software, 67*(1), 1-48. <https://doi.org/10.18637/jss.v067.i01>

R Core Team. (2024). *R: A language and environment for statistical computing*. R Foundation for Statistical Computing. <https://www.R-project.org/>

Therneau, T. M. (2024). *A package for survival analysis in R* (R package version 3.5-8). <https://CRAN.R-project.org/package=survival>

Therneau, T. M. (2024). *coxme: Mixed effects Cox models* (R package version 2.2-20). <https://CRAN.R-project.org/package=coxme>

Therneau, T. M., & Grambsch, P. M. (2000). *Modeling survival data: Extending the Cox model*. Springer. ISBN 0-387-98784-3
